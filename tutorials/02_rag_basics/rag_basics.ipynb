{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure knowledge_base folder exists and has at least one document\n",
    "import os\n",
    "knowledge_base_dir = \"./knowledge_base\"\n",
    "os.makedirs(knowledge_base_dir, exist_ok=True)\n",
    "\n",
    "sample_doc_path = os.path.join(knowledge_base_dir, \"france_rag.txt\")\n",
    "if not os.path.exists(sample_doc_path):\n",
    "    with open(sample_doc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"France is a country in Western Europe. Its capital city is Paris, which is known for its art, culture, and history. Paris is home to landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nRAG (Retrieval-Augmented Generation) is an approach that combines retrieval of relevant documents with generative models to answer questions using both context and model knowledge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64377d",
   "metadata": {},
   "source": [
    "# Tutorial 2: Introduction to RAG (Retrieval-Augmented Generation)\n",
    "This notebook demonstrates the basics of RAG: loading a knowledge base, creating embeddings, storing them in ChromaDB, retrieving relevant chunks, and generating answers using an LLM (Gemini, OpenAI, Claude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918553c8",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "We need ChromaDB (or FAISS), OpenAI, Anthropic, Google GenerativeAI, and dotenv for API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install chromadb openai anthropic google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767126de",
   "metadata": {},
   "source": [
    "## Step 2: Load Knowledge Base\n",
    "Load a small set of local text documents as our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Example: Load text files from a folder\n",
    "knowledge_base_dir = \"./knowledge_base\"\n",
    "documents = []\n",
    "for filename in os.listdir(knowledge_base_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(knowledge_base_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf03679",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings and Store in ChromaDB\n",
    "Generate embeddings for each document and store them in ChromaDB. (Supports OpenAI, Gemini, and Claude embeddings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b346040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select embedding provider: \"openai\", \"gemini\", or \"anthropic\"\n",
    "embedding_provider = \"gemini\"  # Change to \"gemini\" or \"anthropic\" as needed\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(name=\"rag_demo\")\n",
    "\n",
    "embeddings = []\n",
    "if embedding_provider == \"openai\":\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "    collection = client.create_collection(name=\"rag_demo\", embedding_function=openai_ef)\n",
    "    for i, doc in enumerate(documents):\n",
    "        collection.add(documents=[doc], ids=[str(i)])\n",
    "\n",
    "elif embedding_provider == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")\n",
    "        # print(f\"Embedding document {doc_embeddings.embed_query(doc)}\")\n",
    "        embeddings.append(doc_embeddings.embed_query(doc))\n",
    "        collection.add(documents=[doc], ids=[str(i)], embeddings=[embeddings[i]])\n",
    "        print(f\"Added document {i} to collection named: {documents[i]}\")\n",
    "\n",
    "elif embedding_provider == \"anthropic\":\n",
    "    import anthropic\n",
    "    client_a = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    for doc in documents:\n",
    "        emb_response = client_a.embeddings.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # or another Claude embedding model\n",
    "            input=doc\n",
    "        )\n",
    "        emb = emb_response.embedding\n",
    "        embeddings.append(emb)\n",
    "    for i, doc in enumerate(documents):\n",
    "        collection.add(documents=[doc], ids=[str(i)], embeddings=[embeddings[i]])\n",
    "else:\n",
    "    print(\"Invalid embedding provider selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49fb0f8",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve Relevant Chunks for a RAG-Specific Query\n",
    "Given a user query, retrieve the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb61e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does keshav likes?\"\n",
    "if embedding_provider == \"gemini\":\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    query_embedder = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", task_type=\"RETRIEVAL_QUERY\")\n",
    "    query_embedding = query_embedder.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "else:    \n",
    "    results = collection.query(query_texts=[query], n_results=2)\n",
    "retrieved_chunks = results[\"documents\"][0]\n",
    "print(\"Retrieved Chunks:\", retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4a8d2",
   "metadata": {},
   "source": [
    "## Step 5: Pass Context + Query to LLM for Answer Generation\n",
    "Send the retrieved context and query to Gemini, OpenAI, or Claude to generate an answer. Select provider below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b22cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose LLM provider: \"openai\", \"anthropic\", or \"gemini\"\n",
    "llm_provider = \"gemini\"  # Change to \"anthropic\" or \"gemini\" as needed\n",
    "\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "print(\"Prompt:\", query)\n",
    "if llm_provider == \"openai\":\n",
    "    import openai\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(\"OpenAI Response:\", response.choices[0].message[\"content\"])\n",
    "\n",
    "elif llm_provider == \"anthropic\":\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",  # or another Claude model\n",
    "        max_tokens=512,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(\"Claude Response:\", response.content[0].text)\n",
    "\n",
    "elif llm_provider == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "    response = model.generate_content(prompt)\n",
    "    print(\"Gemini Response:\", response.text)\n",
    "else:\n",
    "    print(\"Invalid LLM provider selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c495a1",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Try changing the query and rerun the cells\n",
    "- Experiment with different embedding models or LLM providers\n",
    "- Proceed to the next tutorial for advanced RAG techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
